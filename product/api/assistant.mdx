---
title: "Assistant API"
description: "Creates a model response for a given Assistant."
openapi: "POST /assistant/v1/chat/completions"
---

Creates a model response for a given assistant id, or pass in an Assistant configuration that should be used for your request.

<Info>
  To use the API you need an API key. You can create API Keys in your [Workspace
  settings](https://app.langdock.com/settings/workspace/api). If you want to interact with an
  existing Assistant, make sure to "Share" access to the assistant with the created API Key
  (Assistants > Your Assistant > Share).
</Info>

## Request Parameters

| Parameter     | Type   | Required                              | Description                                    |
| ------------- | ------ | ------------------------------------- | ---------------------------------------------- |
| `assistantId` | string | One of assistantId/assistant required | ID of an existing assistant to use             |
| `assistant`   | object | One of assistantId/assistant required | Configuration for a new assistant              |
| `messages`    | array  | Yes                                   | Array of message objects with role and content |

### Message Format

Each message in the `messages` array should contain:

- `role` (required) - One of: "user", "assistant", or "tool"
- `content` (required) - The message content as a string
- `attachmentIds` (optional) - Array of UUID strings identifying attachments for this message

### Assistant Configuration

When creating a temporary assistant, you can specify:

- `name` (required) - Name of the assistant (max 64 chars)
- `instructions` (required) - System instructions (max 16384 chars)
- `description` - Optional description (max 256 chars)
- `temperature` - Temperature between 0-1
- `model` - Model ID to use (see [Available Models](/product/api/assistant-models) for options)
- `capabilities` - Enable features like web search, data analysis, image generation
- `actions` - Custom API integrations
- `vectorDb` - Vector database connections
- `knowledgeFolderIds` - IDs of knowledge folders to use
- `attachmentIds` - Array of UUID strings identifying attachments to use

<Info>
  You can retrieve a list of available models using the [Models
  API](/product/api/assistant-models). This is useful when you want to see which models you can
  use in your assistant configuration.
</Info>

## Obtaining Attachment IDs

To use attachments in your assistant conversations, you first need to upload the files using the [Upload Attachment API](/product/api/upload-attachments). This will return an `attachmentId` for each file, which you can then include in the `attachmentIds` array in your assistant or message configuration.

## Examples

### Using an Existing Assistant

```javascript
const axios = require("axios");

async function chatWithAssistant() {
  const response = await axios.post(
    "https://api.langdock.com/assistant/v1/chat/completions",
    {
      assistantId: "asst_123",
      messages: [
        {
          role: "user",
          content: "Can you analyze this document for me?",
          attachmentIds: ["550e8400-e29b-41d4-a716-446655440000"], // Obtain attachmentIds from upload attachment endpoint
        },
      ],
    },
    {
      headers: {
        Authorization: "Bearer YOUR_API_KEY",
      },
    }
  );

  console.log(response.data.result);
}
```

### Using a temporary Assistant configuration

```javascript
const axios = require("axios");

async function chatWithNewAssistant() {
  const response = await axios.post(
    "https://api.langdock.com/assistant/v1/chat/completions",
    {
      assistant: {
        name: "Document Analyzer",
        instructions:
          "You are a helpful assistant who analyzes documents and answers questions about them",
        temperature: 0.7,
        model: "gpt-4",
        capabilities: {
          webSearch: true,
          dataAnalyst: true,
        },
        attachmentIds: ["550e8400-e29b-41d4-a716-446655440000"], // Obtain attachmentIds from upload attachment endpoint
      },
      messages: [
        {
          role: "user",
          content: "What are the key points in the document?",
        },
      ],
    },
    {
      headers: {
        Authorization: "Bearer YOUR_API_KEY",
      },
    }
  );

  console.log(response.data.result);
}
```

## Rate limits

The rate limit for the Assistant Completion endpoint is **500 RPM (requests per minute)** and **60.000 TPM (tokens per minute)**. Rate limits are defined at the workspace level - and not at an API key level. Each model has its own rate limit. If you exceed your rate limit, you will receive a `429 Too Many Requests` response.

Please note that the rate limits are subject to change, refer to this documentation for the most up-to-date information.
In case you need a higher rate limit, please contact us at [support@langdock.com](mailto:support@langdock.com).

## Response Format

The API returns an array of results, where each result contains:

```typescript
{
  result: Array<{
    id: string;
    role: "tool" | "assistant";
    content: Array<{
      type: string;
      toolCallId?: string;
      toolName?: string;
      result?: object;
      args?: object;
      text?: string;
    }>;
  }>;
}
```

## Error Handling

```javascript
try {
  const response = await axios.post('https://api.langdock.com/assistant/v1/chat/completions', ...);
} catch (error) {
  if (error.response) {
    switch (error.response.status) {
      case 400:
        console.error('Invalid parameters:', error.response.data.message);
        break;
      case 429:
        console.error('Rate limit exceeded');
        break;
      case 500:
        console.error('Server error');
        break;
    }
  }
}
```
