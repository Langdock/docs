openapi: 3.0.0
info:
  title: Langdock API
  version: 3.0.0
servers:
  - url: https://api.langdock.com
paths:
  /openai/{region}/v1/chat/completions:
    post:
      tags:
        - Chat
      summary: Creates a model response for the given chat conversation.
      parameters:
        - name: region
          in: path
          required: true
          description: The region of the API to use.
          schema:
            type: string
            enum: ["eu", "us"]
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateChatCompletionRequest"
            example:
              model: "gpt-4o-mini"
              messages:
                - role: "system"
                  content: "You are a helpful assistant."
                - role: "user"
                  content: "Write a short poem about cats."
      responses:
        "200":
          description: OK
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/CreateChatCompletionResponse"
              example:
                choices:
                  - message:
                      content: "In moonlit shadows soft they prowl,\nWith eyes aglow in night's dark cowl."
                      role: assistant
                    index: 0
                    finish_reason: "stop"
                    logprobs: null
                created: 1721722200
                id: "chatcmpl-8o4sq3sSzGVqS0aQyjlXuuEGVZnSj"
                model: "gpt-4o-2024-05-13"
                object: "chat.completion"
                system_fingerprint: "fp_asd28019bf"
                usage:
                  completion_tokens: 34
                  prompt_tokens: 14
                  total_tokens: 48
  /anthropic/{region}/v1/messages:
    post:
      tags:
        - Messages
      summary: Create a Message
      description: |-
        Create a Message.

        Send a structured list of input messages with text and/or image content, and the model will generate the next message in the conversation.

        The Messages API can be used for either single queries or stateless multi-turn conversations.
      parameters:
        - name: region
          in: path
          required: true
          description: The region of the API to use.
          schema:
            type: string
            enum: ["eu", "us"]
      operationId: messages_post
      responses:
        "200":
          description: Message object.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Message"
        4XX:
          description: |-
            Error response.

            See Anthropic's [errors documentation](https://docs.anthropic.com/en/api/errors) for more details.
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ErrorResponse"
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateMessageParams"
        required: true
  /mistral/{region}/v1/fim/completions:
    post:
      summary: Fim Completion
      description: FIM completion.
      parameters:
        - name: region
          in: path
          required: true
          description: The region of the API to use.
          schema:
            type: string
            enum: ["eu"]
      operationId: fim_completion_v1_fim_completions_post
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/FIMCompletionRequest"
            example:  
              model: "codestral-2405"
              prompt: "function removeSpecialCharactersWithRegex(str: string) {"
              max_tokens: 64
      responses:
        "200":
          description: Successful Response
          content:
            application/json:
              schema: { $ref: "#/components/schemas/FIMCompletionResponse" }
              example:
                data: "asd"
                id: "245c52bc936f53ba90327800c73d1c3e"
                object: "chat.completion"
                model: "codestral"
                usage:
                  prompt_tokens: 16
                  completion_tokens: 102
                  total_tokens: 118
                created: 1732902806
                choices:
                  - index: 0
                    message:
                      content: "\n  // Use a regular expression to match any non-alphanumeric character and replace it with an empty string\n  return str.replace(/[^a-zA-Z0-9]/g, '');\n}\n\n// Test the function\nconst inputString = \"Hello, World! 123\";\nconst outputString = removeSpecialCharactersWithRegex(inputString);\nconsole.log(outputString); // Output: \"HelloWorld123\""
                      prefix: false
                      role: "assistant"
                    finish_reason: "stop"
            text/event-stream:
              schema:
                $ref: "#/components/schemas/CompletionEvent"
      tags:
        - fim
  /knowledge/search:
    post:
      summary: Search through all files in data folders shared with the API Key
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              required:
                - query
              properties:
                query:
                  type: string
                  description: The search query
                  example: "API Documentation"
      responses:
        "200":
          description: Successfully found search result
      security:
        - bearerAuth: []
  /knowledge/{folderId}:
    post:
      summary: Upload a file to a knowledge folder
      parameters:
        - name: folderId
          in: path
          required: true
          description: The ID of the knowledge folder
          schema:
            type: string
      requestBody:
        required: true
        content:
          multipart/form-data:
            schema:
              type: object
              properties:
                file:
                  type: string
                  format: binary
                  description: The file to upload
                url:
                  type: string
                  description: The associated URL
      responses:
        "200":
          description: File uploaded successfully
      security:
        - bearerAuth: []

    patch:
      summary: Update a file in a knowledge folder
      parameters:
        - name: folderId
          in: path
          required: true
          description: The ID of the knowledge folder
          schema:
            type: string
      requestBody:
        required: true
        content:
          multipart/form-data:
            schema:
              type: object
              properties:
                file:
                  type: string
                  format: binary
                  description: The new file to upload
                url:
                  type: string
                  description: URL that is shown to the user if the file is used in an answer
                attachmentId:
                  type: string
                  description: The ID of the attachment to update
      responses:
        "200":
          description: Attachment updated successfully
      security:
        - bearerAuth: []
  /knowledge/{folderId}/list:
    get:
      summary: Retrieve files from a knowledge folder
      parameters:
        - name: folderId
          in: path
          required: true
          description: The ID of the knowledge folder
          schema:
            type: string
      responses:
        "200":
          description: List of files retrieved successfully
      security:
        - bearerAuth: []
  /knowledge/{folderId}/{attachmentId}:
    delete:
      summary: Delete a file from a knowledge folder
      parameters:
        - name: folderId
          in: path
          required: true
          description: The ID of the knowledge folder
          schema:
            type: string
        - name: attachmentId
          in: path
          required: true
          description: The ID of the attachment to delete
          schema:
            type: string
      responses:
        "200":
          description: Attachment deleted successfully
      security:
        - bearerAuth: []
  /assistant/v1/chat/completions:
    post:
      tags:
        - Assistant
      summary: Creates a chat completion with an assistant
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                assistantId:
                  type: string
                  nullable: true
                  description: ID of an existing assistant to use
                assistant:
                  $ref: "#/components/schemas/Assistant"
                messages:
                  type: array
                  items:
                    type: object
                    required:
                      - role
                      - content
                    properties:
                      role:
                        type: string
                        enum: [user, assistant, tool]
                      content:
                        type: string
                      attachmentIds:
                        type: array
                        items:
                          type: string
                          format: uuid
                        description: Array of UUID strings identifying attachments for this message
                output:
                  type: object
                  description: Specification for structured output format. When type is object/array and no schema is provided, the response will be JSON but can have any structure. When the type is enum, you must provide an enum parameter with an array of strings as options.
                  properties:
                    type:
                      type: string
                      enum: [object, array, enum]
                      description: The type of structured output
                    schema:
                      type: object
                      description: JSON Schema definition for the output (required for object/array types with specific structure). Search for "JSON to JSON Schema" in the web to find a tool to convert any JSON into the required JSON Schema format.
                    enum:
                      type: array
                      items:
                        type: string
                      description: Array of allowed values (required for enum type). Values must be of type string.
                  required:
                    - type
                  oneOf:
                    - properties:
                        type:
                          enum: [enum]
                        enum:
                          type: array
                          items:
                            type: string
                      required: [enum]
                    - properties:
                        type:
                          enum: [object, array]
                        schema:
                          type: object
                    - properties:
                        type:
                          enum: [object, array]
              oneOf:
                - required: [assistantId, messages]
                - required: [assistant, messages]

      responses:
        "200":
          description: Successful chat completion
          content:
            application/json:
              schema:
                type: object
                required:
                  - result
                properties:
                  result:
                    type: array
                    items:
                      type: object
                      required:
                        - id
                        - role
                        - content
                      properties:
                        id:
                          type: string
                        role:
                          type: string
                          enum: [tool, assistant]
                        content:
                          type: array
                          items:
                            type: object
                            required:
                              - type
                            properties:
                              type:
                                type: string
                              toolCallId:
                                type: string
                              toolName:
                                type: string
                              result:
                                type: object
                              args:
                                type: object
                              text:
                                type: string
                  output:
                    description: Present when output parameter was specified in the request
                    oneOf:
                      - type: object
                        description: When output.type is "object"
                      - type: array
                        description: When output.type is "array"
                      - type: string
                        description: When output.type is "enum" (one of the provided enum values)
        "400":
          description: Invalid request parameters
          content:
            application/json:
              schema:
                type: object
                properties:
                  message:
                    oneOf:
                      - type: string
                      - type: array
                        items:
                          type: object
        "429":
          description: Rate limit exceeded
          content:
            application/json:
              schema:
                type: object
                properties:
                  message:
                    type: string
        "500":
          description: Internal server error
          content:
            application/json:
              schema:
                type: object
                properties:
                  message:
                    type: string
  /assistant/v1/models:
    get:
      tags:
        - Assistant
      summary: Lists the available models
      description: Returns a list of models that are available for use with the API.
      responses:
        "200":
          description: List of available models
          content:
            application/json:
              schema:
                type: object
                required:
                  - object
                  - data
                properties:
                  object:
                    type: string
                    enum: [list]
                  data:
                    type: array
                    items:
                      type: object
                      required:
                        - id
                        - object
                        - created
                        - owned_by
                      properties:
                        id:
                          type: string
                          description: The model identifier
                        object:
                          type: string
                          enum: [model]
                          description: The object type, which is always "model"
                        created:
                          type: integer
                          format: int64
                          description: Unix timestamp (in milliseconds) of when the model was created
                        owned_by:
                          type: string
                          enum: [system]
                          description: The organization that owns the model
              example:
                object: "list"
                data:
                  - id: "gpt-4o"
                    object: "model"
                    created: 1686935735000
                    owned_by: "system"
                  - id: "gpt-4o-mini"
                    object: "model"
                    created: 1686935735000
                    owned_by: "system"
        "400":
          description: Invalid request parameters
          content:
            application/json:
              schema:
                type: object
                properties:
                  message:
                    type: array
                    items:
                      type: object
                      description: Validation error details
        "429":
          description: Rate limit exceeded
          content:
            application/json:
              schema:
                type: object
                properties:
                  message:
                    type: string
        "500":
          description: Internal server error
          content:
            application/json:
              schema:
                type: object
                properties:
                  message:
                    type: string
                    example: "Internal Server Error"
  /attachment/v1/upload:
    post:
      operationId: uploadAttachment
      summary: Upload an attachment
      description: Upload a file that can be referenced in Assistant conversations.
      tags: [Attachments]
      requestBody:
        required: true
        content:
          multipart/form-data:
            schema:
              type: object
              required:
                - file
              properties:
                file:
                  type: string
                  format: binary
                  description: The file to upload
      responses:
        "200":
          description: Successfully uploaded file
          content:
            application/json:
              schema:
                type: object
                required:
                  - attachmentId
                  - file
                properties:
                  attachmentId:
                    type: string
                    format: uuid
                    description: Unique identifier for the uploaded attachment
                  file:
                    type: object
                    required:
                      - name
                      - mimeType
                      - sizeInBytes
                    properties:
                      name:
                        type: string
                        description: Original filename
                      mimeType:
                        type: string
                        description: MIME type of the file
                      sizeInBytes:
                        type: integer
                        description: Size of the file in bytes
        "400":
          description: No file provided
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
        "401":
          description: Invalid API key
          content:
            application/json:
              schema:
                type: object
                properties:
                  error:
                    type: string
        "500":
          description: Internal server error
          content:
            application/json:
              schema:
                type: object
                properties:
                  message:
                    type: string
components:
  schemas:
    # OpenAI schemas
    ChatCompletionRequestMessageContentPart:
      oneOf:
        - $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartText"
        - $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartImage"
      x-oaiExpandable: true
    ChatCompletionRequestMessageContentPartImage:
      type: object
      title: Image content part
      properties:
        type:
          type: string
          enum: ["image_url"]
          description: The type of the content part.
        image_url:
          type: object
          properties:
            url:
              type: string
              description: Either a URL of the image or the base64 encoded image data.
              format: uri
            detail:
              type: string
              description: Specifies the detail level of the image. Learn more in the [Vision guide](https://platform.openai.com/docs/guides/vision/low-or-high-fidelity-image-understanding).
              enum: ["auto", "low", "high"]
              default: "auto"
          required:
            - url
      required:
        - type
        - image_url
    ChatCompletionRequestMessageContentPartText:
      type: object
      title: Text content part
      properties:
        type:
          type: string
          enum: ["text"]
          description: The type of the content part.
        text:
          type: string
          description: The text content.
      required:
        - type
        - text
    ChatCompletionRequestMessage:
      oneOf:
        - $ref: "#/components/schemas/ChatCompletionRequestSystemMessage"
        - $ref: "#/components/schemas/ChatCompletionRequestUserMessage"
        - $ref: "#/components/schemas/ChatCompletionRequestAssistantMessage"
        - $ref: "#/components/schemas/ChatCompletionRequestToolMessage"
        - $ref: "#/components/schemas/ChatCompletionRequestFunctionMessage"
      x-oaiExpandable: true
    ChatCompletionRequestSystemMessage:
      type: object
      title: System message
      properties:
        content:
          description: The contents of the system message.
          type: string
        role:
          type: string
          enum: ["system"]
          description: The role of the messages author, in this case `system`.
        name:
          type: string
          description: An optional name for the participant. Provides the model information to differentiate between participants of the same role.
      required:
        - content
        - role
    ChatCompletionRequestUserMessage:
      type: object
      title: User message
      properties:
        content:
          description: |
            The contents of the user message.
          oneOf:
            - type: string
              description: The text contents of the message.
              title: Text content
            - type: array
              description: An array of content parts with a defined type, each can be of type `text` or `image_url` when passing in images. You can pass multiple images by adding multiple `image_url` content parts. Image input is only supported when using the `gpt-4-visual-preview` model.
              title: Array of content parts
              items:
                $ref: "#/components/schemas/ChatCompletionRequestMessageContentPart"
              minItems: 1
          x-oaiExpandable: true
        role:
          type: string
          enum: ["user"]
          description: The role of the messages author, in this case `user`.
        name:
          type: string
          description: An optional name for the participant. Provides the model information to differentiate between participants of the same role.
      required:
        - content
        - role
    ChatCompletionRequestAssistantMessage:
      type: object
      title: Assistant message
      properties:
        content:
          nullable: true
          type: string
          description: |
            The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified.
        role:
          type: string
          enum: ["assistant"]
          description: The role of the messages author, in this case `assistant`.
        name:
          type: string
          description: An optional name for the participant. Provides the model information to differentiate between participants of the same role.
        tool_calls:
          $ref: "#/components/schemas/ChatCompletionMessageToolCalls"
        function_call:
          type: object
          deprecated: true
          description: "Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model."
          nullable: true
          properties:
            arguments:
              type: string
              description: The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
            name:
              type: string
              description: The name of the function to call.
          required:
            - arguments
            - name
      required:
        - role
    ChatCompletionRequestToolMessage:
      type: object
      title: Tool message
      properties:
        role:
          type: string
          enum: ["tool"]
          description: The role of the messages author, in this case `tool`.
        content:
          type: string
          description: The contents of the tool message.
        tool_call_id:
          type: string
          description: Tool call that this message is responding to.
      required:
        - role
        - content
        - tool_call_id
    ChatCompletionRequestFunctionMessage:
      type: object
      title: Function message
      deprecated: true
      properties:
        role:
          type: string
          enum: ["function"]
          description: The role of the messages author, in this case `function`.
        content:
          nullable: true
          type: string
          description: The contents of the function message.
        name:
          type: string
          description: The name of the function to call.
      required:
        - role
        - content
        - name
    FunctionParameters:
      type: object
      description: "The parameters the functions accepts, described as a JSON Schema object. See the [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format. \n\nOmitting `parameters` defines a function with an empty parameter list."
      additionalProperties: true
    ChatCompletionFunctions:
      type: object
      deprecated: true
      properties:
        description:
          type: string
          description: A description of what the function does, used by the model to choose when and how to call the function.
        name:
          type: string
          description: The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
        parameters:
          $ref: "#/components/schemas/FunctionParameters"
      required:
        - name
    ChatCompletionFunctionCallOption:
      type: object
      description: >
        Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.
      properties:
        name:
          type: string
          description: The name of the function to call.
      required:
        - name
    ChatCompletionTool:
      type: object
      properties:
        type:
          type: string
          enum: ["function"]
          description: The type of the tool. Currently, only `function` is supported.
        function:
          $ref: "#/components/schemas/FunctionObject"
      required:
        - type
        - function
    FunctionObject:
      type: object
      properties:
        description:
          type: string
          description: A description of what the function does, used by the model to choose when and how to call the function.
        name:
          type: string
          description: The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
        parameters:
          $ref: "#/components/schemas/FunctionParameters"
      required:
        - name
    ChatCompletionToolChoiceOption:
      description: |
        Controls which (if any) tool is called by the model.
        `none` means the model will not call any tool and instead generates a message.
        `auto` means the model can pick between generating a message or calling one or more tools.
        `required` means the model must call one or more tools.
        Specifying a particular tool via `{"type": "function", "function": {"name": "my_function"}}` forces the model to call that tool.

        `none` is the default when no tools are present. `auto` is the default if tools are present.
      oneOf:
        - type: string
          description: >
            `none` means the model will not call any tool and instead generates a message.
            `auto` means the model can pick between generating a message or calling one or more tools.
            `required` means the model must call one or more tools.
          enum: [none, auto, required]
        - $ref: "#/components/schemas/ChatCompletionNamedToolChoice"
      x-oaiExpandable: true
    ChatCompletionNamedToolChoice:
      type: object
      description: Specifies a tool the model should use. Use to force the model to call a specific function.
      properties:
        type:
          type: string
          enum: ["function"]
          description: The type of the tool. Currently, only `function` is supported.
        function:
          type: object
          properties:
            name:
              type: string
              description: The name of the function to call.
          required:
            - name
      required:
        - type
        - function
    ChatCompletionMessageToolCalls:
      type: array
      description: The tool calls generated by the model, such as function calls.
      items:
        $ref: "#/components/schemas/ChatCompletionMessageToolCall"
    ChatCompletionMessageToolCall:
      type: object
      properties:
        # TODO: index included when streaming
        id:
          type: string
          description: The ID of the tool call.
        type:
          type: string
          enum: ["function"]
          description: The type of the tool. Currently, only `function` is supported.
        function:
          type: object
          description: The function that the model called.
          properties:
            name:
              type: string
              description: The name of the function to call.
            arguments:
              type: string
              description: The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
          required:
            - name
            - arguments
      required:
        - id
        - type
        - function
    ChatCompletionResponseMessage:
      type: object
      description: A chat completion message generated by the model.
      properties:
        content:
          type: string
          description: The contents of the message.
          nullable: true
        tool_calls:
          $ref: "#/components/schemas/ChatCompletionMessageToolCalls"
        role:
          type: string
          enum: ["assistant"]
          description: The role of the author of this message.
        function_call:
          type: object
          deprecated: true
          description: "Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model."
          properties:
            arguments:
              type: string
              description: The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
            name:
              type: string
              description: The name of the function to call.
          required:
            - name
            - arguments
      required:
        - role
        - content
    CreateCompletionRequest:
      type: object
      properties:
        model:
          description: &model_description |
            ID of the model to use. You can use the [List models](https://platform.openai.com/docs/api-reference/models/list) API to see all of your available models, or see OpenAI's [Model overview](https://platform.openai.com/docs/models/overview) for descriptions of them.
          anyOf:
            - type: string
            - type: string
              enum: ["gpt-3.5-turbo-instruct", "davinci-002", "babbage-002"]
          x-oaiTypeLabel: string
        prompt:
          description: &completions_prompt_description |
            The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.

            Note that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.
          default: "<|endoftext|>"
          nullable: true
          oneOf:
            - type: string
              default: ""
              example: "This is a test."
            - type: array
              items:
                type: string
                default: ""
                example: "This is a test."
            - type: array
              minItems: 1
              items:
                type: integer
              example: "[1212, 318, 257, 1332, 13]"
            - type: array
              minItems: 1
              items:
                type: array
                minItems: 1
                items:
                  type: integer
              example: "[[1212, 318, 257, 1332, 13]]"
        best_of:
          type: integer
          default: 1
          minimum: 0
          maximum: 20
          nullable: true
          description: &completions_best_of_description |
            Generates `best_of` completions server-side and returns the "best" (the one with the highest log probability per token). Results cannot be streamed.

            When used with `n`, `best_of` controls the number of candidate completions and `n` specifies how many to return – `best_of` must be greater than `n`.

            **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
        echo:
          type: boolean
          default: false
          nullable: true
          description: &completions_echo_description >
            Echo back the prompt in addition to the completion
        frequency_penalty:
          type: number
          default: 0
          minimum: -2
          maximum: 2
          nullable: true
          description: &completions_frequency_penalty_description |
            Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.

            [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation/parameter-details)
        logit_bias: &completions_logit_bias
          type: object
          x-oaiTypeLabel: map
          default: null
          nullable: true
          additionalProperties:
            type: integer
          description: &completions_logit_bias_description |
            Modify the likelihood of specified tokens appearing in the completion.

            Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.

            As an example, you can pass `{"50256": -100}` to prevent the <|endoftext|> token from being generated.
        logprobs: &completions_logprobs_configuration
          type: integer
          minimum: 0
          maximum: 5
          default: null
          nullable: true
          description: &completions_logprobs_description |
            Include the log probabilities on the `logprobs` most likely output tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.

            The maximum value for `logprobs` is 5.
        max_tokens:
          type: integer
          minimum: 0
          default: 16
          example: 16
          nullable: true
          description: &completions_max_tokens_description |
            The maximum number of [tokens](/tokenizer) that can be generated in the completion.

            The token count of your prompt plus `max_tokens` cannot exceed the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.
        presence_penalty:
          type: number
          default: 0
          minimum: -2
          maximum: 2
          nullable: true
          description: &completions_presence_penalty_description |
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.

            [See more information about frequency and presence penalties.](https://platform.openai.com/docs/guides/text-generation/parameter-details)
        seed: &completions_seed_param
          type: integer
          minimum: -9223372036854775808
          maximum: 9223372036854775807
          nullable: true
          description: |
            If specified, OpenAI's system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.

            Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
        stop:
          description: &completions_stop_description >
            Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
          default: null
          nullable: true
          oneOf:
            - type: string
              default: <|endoftext|>
              example: "\n"
              nullable: true
            - type: array
              minItems: 1
              maxItems: 4
              items:
                type: string
                example: '["\n"]'
        stream:
          description: >
            Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
            as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).
          type: boolean
          nullable: true
          default: false
        suffix:
          description: |
            The suffix that comes after a completion of inserted text.

            This parameter is only supported for `gpt-3.5-turbo-instruct`.
          default: null
          nullable: true
          type: string
          example: "test."
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 1
          example: 1
          nullable: true
          description: &completions_temperature_description |
            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.

            We generally recommend altering this or `top_p` but not both.
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: &completions_top_p_description |
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.

            We generally recommend altering this or `temperature` but not both.
        user: &end_user_param_configuration
          type: string
          example: user-1234
          description: |
            A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids).
      required:
        - model
        - prompt
    CreateChatCompletionRequest:
      type: object
      properties:
        messages:
          description: A list of messages comprising the conversation so far. [Example Python code](https://cookbook.openai.com/examples/how_to_format_inputs_to_chatgpt_models).
          type: array
          minItems: 1
          items:
            $ref: "#/components/schemas/ChatCompletionRequestMessage"
        model:
          description: ID of the model to use. See the [model endpoint compatibility](https://platform.openai.com/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API.
          example: "gpt-4-turbo"
          anyOf:
            - type: string
            - type: string
              enum: ["gpt-4o", "gpt-4o-mini", "gpt-4", "gpt-3.5-turbo"]
          x-oaiTypeLabel: string
        frequency_penalty:
          type: number
          default: 0
          minimum: -2
          maximum: 2
          nullable: true
          description: *completions_frequency_penalty_description
        logit_bias:
          type: object
          x-oaiTypeLabel: map
          default: null
          nullable: true
          additionalProperties:
            type: integer
          description: |
            Modify the likelihood of specified tokens appearing in the completion.

            Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
        logprobs:
          description: Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.
          type: boolean
          default: false
          nullable: true
        top_logprobs:
          description: An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.
          type: integer
          minimum: 0
          maximum: 20
          nullable: true
        max_tokens:
          description: |
            The maximum number of [tokens](/tokenizer) that can be generated in the chat completion.

            The total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.
          type: integer
          nullable: true
        presence_penalty:
          type: number
          default: 0
          minimum: -2
          maximum: 2
          nullable: true
          description: *completions_presence_penalty_description
        response_format:
          type: object
          description: |
            An object specifying the format that the model must output. Compatible with GPT-4 Turbo and all GPT-3.5 Turbo models newer than `gpt-3.5-turbo-1106`.

            Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.

            **Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if `finish_reason="length"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.
          properties:
            type:
              type: string
              enum: ["text", "json_object"]
              example: "json_object"
              default: "text"
              description: Must be one of `text` or `json_object`.
        seed:
          type: integer
          minimum: -9223372036854775808
          maximum: 9223372036854775807
          nullable: true
          description: |
            This feature is in Beta.
            If specified, OpenAI's system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.
            Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
          x-oaiMeta:
            beta: true
        stop:
          description: |
            Up to 4 sequences where the API will stop generating further tokens.
          default: null
          oneOf:
            - type: string
              nullable: true
            - type: array
              minItems: 1
              maxItems: 4
              items:
                type: string
        stream:
          description: >
            If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
            as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).
          type: boolean
          nullable: true
          default: false
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 1
          example: 1
          nullable: true
          description: *completions_temperature_description
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: *completions_top_p_description
        tools:
          type: array
          description: >
            A list of tools the model may call. Currently, only functions are supported as a tool.
            Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.
          items:
            $ref: "#/components/schemas/ChatCompletionTool"
        tool_choice:
          $ref: "#/components/schemas/ChatCompletionToolChoiceOption"
        user: *end_user_param_configuration
        function_call:
          deprecated: true
          description: |
            Deprecated in favor of `tool_choice`.

            Controls which (if any) function is called by the model.
            `none` means the model will not call a function and instead generates a message.
            `auto` means the model can pick between generating a message or calling a function.
            Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.

            `none` is the default when no functions are present. `auto` is the default if functions are present.
          oneOf:
            - type: string
              description: >
                `none` means the model will not call a function and instead generates a message.
                `auto` means the model can pick between generating a message or calling a function.
              enum: [none, auto]
            - $ref: "#/components/schemas/ChatCompletionFunctionCallOption"
          x-oaiExpandable: true
        functions:
          deprecated: true
          description: |
            Deprecated in favor of `tools`.

            A list of functions the model may generate JSON inputs for.
          type: array
          minItems: 1
          maxItems: 128
          items:
            $ref: "#/components/schemas/ChatCompletionFunctions"

      required:
        - model
        - messages
    CreateChatCompletionResponse:
      type: object
      description: Represents a chat completion response returned by model, based on the provided input.
      properties:
        id:
          type: string
          description: A unique identifier for the chat completion.
        choices:
          type: array
          description: A list of chat completion choices. Can be more than one if `n` is greater than 1.
          items:
            type: object
            required:
              - finish_reason
              - index
              - message
              - logprobs
            properties:
              finish_reason:
                type: string
                description: &chat_completion_finish_reason_description |
                  The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
                  `length` if the maximum number of tokens specified in the request was reached,
                  `content_filter` if content was omitted due to a flag from OpenAI's content filters,
                  `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.
                enum: ["stop", "length", "tool_calls", "content_filter", "function_call"]
              index:
                type: integer
                description: The index of the choice in the list of choices.
              message:
                $ref: "#/components/schemas/ChatCompletionResponseMessage"
              logprobs: &chat_completion_response_logprobs
                description: Log probability information for the choice.
                type: object
                nullable: true
                properties:
                  content:
                    description: A list of message content tokens with log probability information.
                    type: array
                    items:
                      $ref: "#/components/schemas/ChatCompletionTokenLogprob"
                    nullable: true
                required:
                  - content
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the chat completion was created.
        model:
          type: string
          description: The model used for the chat completion.
        system_fingerprint:
          type: string
          description: |
            This fingerprint represents the backend configuration that the model runs with.

            Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
        object:
          type: string
          description: The object type, which is always `chat.completion`.
          enum: [chat.completion]
        usage:
          $ref: "#/components/schemas/CompletionUsage"
      required:
        - choices
        - created
        - id
        - model
        - object
      x-oaiMeta:
        name: The chat completion object
        group: chat
        example: &chat_completion_example |
          {
            "id": "chatcmpl-123",
            "object": "chat.completion",
            "created": 1677652288,
            "model": "gpt-4o-mini",
            "system_fingerprint": "fp_44709d6fcb",
            "choices": [{
              "index": 0,
              "message": {
                "role": "assistant",
                "content": "\n\nHello there, how may I assist you today?",
              },
              "logprobs": null,
              "finish_reason": "stop"
            }],
            "usage": {
              "prompt_tokens": 9,
              "completion_tokens": 12,
              "total_tokens": 21
            }
    ChatCompletionTokenLogprob:
      type: object
      properties:
        token: &chat_completion_response_logprobs_token
          description: The token.
          type: string
        logprob: &chat_completion_response_logprobs_token_logprob
          description: The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
          type: number
        bytes: &chat_completion_response_logprobs_bytes
          description: A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token.
          type: array
          items:
            type: integer
          nullable: true
        top_logprobs:
          description: List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested `top_logprobs` returned.
          type: array
          items:
            type: object
            properties:
              token: *chat_completion_response_logprobs_token
              logprob: *chat_completion_response_logprobs_token_logprob
              bytes: *chat_completion_response_logprobs_bytes
            required:
              - token
              - logprob
              - bytes
      required:
        - token
        - logprob
        - bytes
        - top_logprobs
    CompletionUsage:
      type: object
      description: Usage statistics for the completion request.
      properties:
        completion_tokens:
          type: integer
          description: Number of tokens in the generated completion.
        prompt_tokens:
          type: integer
          description: Number of tokens in the prompt.
        total_tokens:
          type: integer
          description: Total number of tokens used in the request (prompt + completion).
      required:
        - prompt_tokens
        - completion_tokens
        - total_tokens
    #============== Anthropic schemas ==============
    APIError:
      properties:
        type:
          default: api_error
          enum:
            - api_error
          title: Type
          type: string
        message:
          default: Internal server error
          title: Message
          type: string
      required:
        - type
        - message
      title: APIError
      type: object
    AuthenticationError:
      properties:
        type:
          default: authentication_error
          enum:
            - authentication_error
          title: Type
          type: string
        message:
          default: Authentication error
          title: Message
          type: string
      required:
        - type
        - message
      title: AuthenticationError
      type: object
    Base64ImageSource:
      additionalProperties: false
      properties:
        type:
          enum:
            - base64
          title: Type
          type: string
        media_type:
          enum:
            - image/jpeg
            - image/png
            - image/gif
            - image/webp
          title: Media Type
          type: string
        data:
          format: byte
          title: Data
          type: string
      required:
        - type
        - media_type
        - data
      title: Base64ImageSource
      type: object
    CreateMessageParams:
      additionalProperties: false
      example:
        max_tokens: 1024
        messages:
          - content: Write a haiku about cats.
            role: user
        model: claude-3-haiku-20240307
      properties:
        model:
          $ref: "#/components/schemas/Model"
        messages:
          description: |-
            Input messages.

            Anthropic's models are trained to operate on alternating `user` and `assistant` conversational turns. When creating a new `Message`, you specify the prior conversational turns with the `messages` parameter, and the model then generates the next `Message` in the conversation.

            Each input message must be an object with a `role` and `content`. You can specify a single `user`-role message, or you can include multiple `user` and `assistant` messages. The first message must always use the `user` role.

            If the final message uses the `assistant` role, the response content will continue immediately from the content in that message. This can be used to constrain part of the model's response.

            Example with a single `user` message:

            ```json
            [{"role": "user", "content": "Hello, Claude"}]
            ```

            Example with multiple conversational turns:

            ```json
            [
              {"role": "user", "content": "Hello there."},
              {"role": "assistant", "content": "Hi, I'm Claude. How can I help you?"},
              {"role": "user", "content": "Can you explain LLMs in plain English?"},
            ]
            ```

            Example with a partially-filled response from Claude:

            ```json
            [
              {"role": "user", "content": "What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun"},
              {"role": "assistant", "content": "The best answer is ("},
            ]
            ```

            Each input message `content` may be either a single `string` or an array of content blocks, where each block has a specific `type`. Using a `string` for `content` is shorthand for an array of one content block of type `"text"`. The following input messages are equivalent:

            ```json
            {"role": "user", "content": "Hello, Claude"}
            ```

            ```json
            {"role": "user", "content": [{"type": "text", "text": "Hello, Claude"}]}
            ```

            Starting with Claude 3 models, you can also send image content blocks:

            ```json
            {"role": "user", "content": [
              {
                "type": "image",
                "source": {
                  "type": "base64",
                  "media_type": "image/jpeg",
                  "data": "/9j/4AAQSkZJRg...",
                }
              },
              {"type": "text", "text": "What is in this image?"}
            ]}
            ```

            We currently support the `base64` source type for images, and the `image/jpeg`, `image/png`, `image/gif`, and `image/webp` media types.

            See [examples](https://docs.anthropic.com/en/api/messages-examples#vision) for more input examples.

            Note that if you want to include a [system prompt](https://docs.anthropic.com/en/docs/system-prompts), you can use the top-level `system` parameter â€" there is no `"system"` role for input messages in the Messages API.
          items:
            $ref: "#/components/schemas/InputMessage"
          title: Messages
          type: array
        max_tokens:
          description: |-
            The maximum number of tokens to generate before stopping.

            Note that Anthropic's models may stop _before_ reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate.

            Different models have different maximum values for this parameter.  See [models](https://docs.anthropic.com/en/docs/models-overview) for details.
          example:
            - 1024
          minimum: 1
          title: Max Tokens
          type: integer
        metadata:
          allOf:
            - $ref: "#/components/schemas/Metadata"
          description: An object describing metadata about the request.
        stop_sequences:
          description: |-
            Custom text sequences that will cause the model to stop generating.

            Anthropic's models will normally stop when they have naturally completed their turn, which will result in a response `stop_reason` of `"end_turn"`.

            If you want the model to stop generating when it encounters custom strings of text, you can use the `stop_sequences` parameter. If the model encounters one of the custom sequences, the response `stop_reason` value will be `"stop_sequence"` and the response `stop_sequence` value will contain the matched stop sequence.
          items:
            type: string
          title: Stop Sequences
          type: array
        stream:
          description: |-
            Whether to incrementally stream the response using server-sent events.

            See [streaming](https://docs.anthropic.com/en/api/messages-streaming) for details.
          title: Stream
          type: boolean
        system:
          anyOf:
            - type: string
              x-stainless-skip:
                - go
            - items:
                $ref: "#/components/schemas/RequestTextBlock"
              type: array
          description: |-
            System prompt.

            A system prompt is a way of providing context and instructions to Claude, such as specifying a particular goal or role. See Anthropic's [guide to system prompts](https://docs.anthropic.com/en/docs/system-prompts).
          example:
            - - text: Today's date is 2024-06-01.
                type: text
            - Today's date is 2023-01-01.
          title: System
        temperature:
          description: |-
            Amount of randomness injected into the response.

            Defaults to `1.0`. Ranges from `0.0` to `1.0`. Use `temperature` closer to `0.0` for analytical / multiple choice, and closer to `1.0` for creative and generative tasks.

            Note that even with `temperature` of `0.0`, the results will not be fully deterministic.
          example:
            - 1
          maximum: 1
          minimum: 0
          title: Temperature
          type: number
        tool_choice:
          description: How the model should use the provided tools. The model can use a specific tool, any available tool, or decide by itself.
          discriminator:
            mapping:
              any: "#/components/schemas/ToolChoiceAny"
              auto: "#/components/schemas/ToolChoiceAuto"
              tool: "#/components/schemas/ToolChoiceTool"
            propertyName: type
          oneOf:
            - $ref: "#/components/schemas/ToolChoiceAuto"
            - $ref: "#/components/schemas/ToolChoiceAny"
            - $ref: "#/components/schemas/ToolChoiceTool"
          title: Tool Choice
        tools:
          description: |-
            Definitions of tools that the model may use.

            If you include `tools` in your API request, the model may return `tool_use` content blocks that represent the model's use of those tools. You can then run those tools using the tool input generated by the model and then optionally return results back to the model using `tool_result` content blocks.

            Each tool definition includes:

            * `name`: Name of the tool.
            * `description`: Optional, but strongly-recommended description of the tool.
            * `input_schema`: [JSON schema](https://json-schema.org/) for the tool `input` shape that the model will produce in `tool_use` output content blocks.

            For example, if you defined `tools` as:

            ```json
            [
              {
                "name": "get_stock_price",
                "description": "Get the current stock price for a given ticker symbol.",
                "input_schema": {
                  "type": "object",
                  "properties": {
                    "ticker": {
                      "type": "string",
                      "description": "The stock ticker symbol, e.g. AAPL for Apple Inc."
                    }
                  },
                  "required": ["ticker"]
                }
              }
            ]
            ```

            And then asked the model "What's the S&P 500 at today?", the model might produce `tool_use` content blocks in the response like this:

            ```json
            [
              {
                "type": "tool_use",
                "id": "toolu_01D7FLrfh4GYq7yT1ULFeyMV",
                "name": "get_stock_price",
                "input": { "ticker": "^GSPC" }
              }
            ]
            ```

            You might then run your `get_stock_price` tool with `{"ticker": "^GSPC"}` as an input, and return the following back to the model in a subsequent `user` message:

            ```json
            [
              {
                "type": "tool_result",
                "tool_use_id": "toolu_01D7FLrfh4GYq7yT1ULFeyMV",
                "content": "259.75 USD"
              }
            ]
            ```

            Tools can be used for workflows that include running client-side tools and functions, or more generally whenever you want the model to produce a particular JSON structure of output.

            See Anthropic's [guide](https://docs.anthropic.com/en/docs/tool-use) for more details.
          example:
            - description: Get the current weather in a given location
              input_schema:
                properties:
                  location:
                    description: The city and state, e.g. San Francisco, CA
                    type: string
                  unit:
                    description: Unit for the output - one of (celsius, fahrenheit)
                    type: string
                required:
                  - location
                type: object
              name: get_weather
          items:
            $ref: "#/components/schemas/Tool"
          title: Tools
          type: array
        top_k:
          description: |-
            Only sample from the top K options for each subsequent token.

            Used to remove "long tail" low probability responses. [Learn more technical details here](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277).

            Recommended for advanced use cases only. You usually only need to use `temperature`.
          example:
            - 5
          minimum: 0
          title: Top K
          type: integer
        top_p:
          description: |-
            Use nucleus sampling.

            In nucleus sampling, we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by `top_p`. You should either alter `temperature` or `top_p`, but not both.

            Recommended for advanced use cases only. You usually only need to use `temperature`.
          example:
            - 0.7
          maximum: 1
          minimum: 0
          title: Top P
          type: number
      required:
        - model
        - messages
        - max_tokens
      title: CreateMessageParams
      type: object
    ErrorResponse:
      properties:
        type:
          default: error
          enum:
            - error
          title: Type
          type: string
        error:
          discriminator:
            mapping:
              api_error: "#/components/schemas/APIError"
              authentication_error: "#/components/schemas/AuthenticationError"
              invalid_request_error: "#/components/schemas/InvalidRequestError"
              not_found_error: "#/components/schemas/NotFoundError"
              overloaded_error: "#/components/schemas/OverloadedError"
              permission_error: "#/components/schemas/PermissionError"
              rate_limit_error: "#/components/schemas/RateLimitError"
            propertyName: type
          oneOf:
            - $ref: "#/components/schemas/InvalidRequestError"
            - $ref: "#/components/schemas/AuthenticationError"
            - $ref: "#/components/schemas/PermissionError"
            - $ref: "#/components/schemas/NotFoundError"
            - $ref: "#/components/schemas/RateLimitError"
            - $ref: "#/components/schemas/APIError"
            - $ref: "#/components/schemas/OverloadedError"
          title: Error
      required:
        - type
        - error
      title: ErrorResponse
      type: object
    InputMessage:
      additionalProperties: false
      properties:
        role:
          enum:
            - user
            - assistant
          title: Role
          type: string
        content:
          anyOf:
            - type: string
              x-stainless-skip:
                - go
            - items:
                discriminator:
                  mapping:
                    image: "#/components/schemas/RequestImageBlock"
                    text: "#/components/schemas/RequestTextBlock"
                    tool_result: "#/components/schemas/RequestToolResultBlock"
                    tool_use: "#/components/schemas/RequestToolUseBlock"
                  propertyName: type
                oneOf:
                  - $ref: "#/components/schemas/RequestTextBlock"
                  - $ref: "#/components/schemas/RequestImageBlock"
                  - $ref: "#/components/schemas/RequestToolUseBlock"
                  - $ref: "#/components/schemas/RequestToolResultBlock"
                x-stainless-python-extend-union:
                  - ContentBlock
                x-stainless-python-extend-union-imports:
                  - from .content_block import ContentBlock
              type: array
              example:
                - type: text
                  text: What is a quaternion?
          title: Content
      required:
        - role
        - content
      title: InputMessage
      type: object
    InputSchema:
      additionalProperties: true
      properties:
        type:
          enum:
            - object
          title: Type
          type: string
        properties:
          anyOf:
            - type: object
          default: null
          title: Properties
      required:
        - type
      title: InputSchema
      type: object
    InvalidRequestError:
      properties:
        type:
          default: invalid_request_error
          enum:
            - invalid_request_error
          title: Type
          type: string
        message:
          default: Invalid request
          title: Message
          type: string
      required:
        - type
        - message
      title: InvalidRequestError
      type: object
    Message:
      example:
        - content:
            - text: Here is a haiku about cats:\n\nFeline grace and charm,\nPurring softly by the fire,\nCats reign supreme.
              type: text
          id: msg_013Zva2CMHLNnXjNJJKqJ2EF
          model: claude-3-haiku-20240307
          role: assistant
          stop_reason: end_turn
          stop_sequence: null
          type: message
          usage:
            input_tokens: 14
            output_tokens: 35
      properties:
        id:
          description: |-
            Unique object identifier.

            The format and length of IDs may change over time.
          example:
            - msg_013Zva2CMHLNnXjNJJKqJ2EF
          title: Id
          type: string
        type:
          default: message
          description: |-
            Object type.

            For Messages, this is always `"message"`.
          enum:
            - message
          title: Type
          type: string
        role:
          default: assistant
          description: |-
            Conversational role of the generated message.

            This will always be `"assistant"`.
          enum:
            - assistant
          title: Role
          type: string
        content:
          description: |-
            Content generated by the model.

            This is an array of content blocks, each of which has a `type` that determines its shape.

            Example:

            ```json
            [{"type": "text", "text": "Hi, I'm Claude."}]
            ```

            If the request input `messages` ended with an `assistant` turn, then the response `content` will continue directly from that last turn. You can use this to constrain the model's output.

            For example, if the input `messages` were:
            ```json
            [
              {"role": "user", "content": "What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun"},
              {"role": "assistant", "content": "The best answer is ("}
            ]
            ```

            Then the response `content` might be:

            ```json
            [{"type": "text", "text": "B)"}]
            ```
          example:
            - - text: Hi! My name is Claude.
                type: text
          items:
            $ref: "#/components/schemas/ContentBlock"
          title: Content
          type: array
        model:
          $ref: "#/components/schemas/Model"
        stop_reason:
          anyOf:
            - enum:
                - end_turn
                - max_tokens
                - stop_sequence
                - tool_use
              type: string

          description: |-
            The reason that we stopped.

            This may be one the following values:
            * `"end_turn"`: the model reached a natural stopping point
            * `"max_tokens"`: we exceeded the requested `max_tokens` or the model's maximum
            * `"stop_sequence"`: one of your provided custom `stop_sequences` was generated
            * `"tool_use"`: the model invoked one or more tools

            In non-streaming mode this value is always non-null. In streaming mode, it is null in the `message_start` event and non-null otherwise.
          title: Stop Reason
        stop_sequence:
          anyOf:
            - type: string

          default: null
          description: |-
            Which custom stop sequence was generated, if any.

            This value will be a non-null string if one of your custom stop sequences was generated.
          title: Stop Sequence
        usage:
          allOf:
            - $ref: "#/components/schemas/Usage"
          description: |-
            Input and output token counts, representing the underlying cost to our systems.

            Under the hood, the API transforms requests into a format suitable for the model. The model's output then goes through a parsing stage before becoming an API response. As a result, the token counts in `usage` will not match one-to-one with the exact visible content of an API request or response.

            For example, `output_tokens` will be non-zero, even for an empty string response from Claude.
          example:
            - input_tokens: 2095
              output_tokens: 503
      required:
        - id
        - type
        - role
        - content
        - model
        - stop_reason
        - stop_sequence
        - usage
      title: Message
      type: object
      x-stainless-python-custom-imports:
        - from .content_block import ContentBlock as ContentBlock
    Metadata:
      additionalProperties: false
      properties:
        user_id:
          anyOf:
            - maxLength: 256
              type: string

          description: |-
            An external identifier for the user who is associated with the request.

            This should be a uuid, hash value, or other opaque identifier. Anthropic may use this id to help detect abuse. Do not include any identifying information such as name, email address, or phone number.
          example:
            - 13803d75-b4b5-4c3e-b2a2-6f21399b021b
          title: User Id
      title: Metadata
      type: object
    NotFoundError:
      properties:
        type:
          default: not_found_error
          enum:
            - not_found_error
          title: Type
          type: string
        message:
          default: Not found
          title: Message
          type: string
      required:
        - type
        - message
      title: NotFoundError
      type: object
    OverloadedError:
      properties:
        type:
          default: overloaded_error
          enum:
            - overloaded_error
          title: Type
          type: string
        message:
          default: Overloaded
          title: Message
          type: string
      required:
        - type
        - message
      title: OverloadedError
      type: object
    PermissionError:
      properties:
        type:
          default: permission_error
          enum:
            - permission_error
          title: Type
          type: string
        message:
          default: Permission denied
          title: Message
          type: string
      required:
        - type
        - message
      title: PermissionError
      type: object
    RateLimitError:
      properties:
        type:
          default: rate_limit_error
          enum:
            - rate_limit_error
          title: Type
          type: string
        message:
          default: Rate limited
          title: Message
          type: string
      required:
        - type
        - message
      title: RateLimitError
      type: object
    RequestImageBlock:
      additionalProperties: false
      properties:
        type:
          enum:
            - image
          title: Type
          type: string
        source:
          discriminator:
            mapping:
              base64: "#/components/schemas/Base64ImageSource"
            propertyName: type
          oneOf:
            - $ref: "#/components/schemas/Base64ImageSource"
          title: Source
      required:
        - type
        - source
      title: Image
      type: object
    RequestTextBlock:
      additionalProperties: false
      properties:
        type:
          enum:
            - text
          title: Type
          type: string
        text:
          minLength: 1
          title: Text
          type: string
      required:
        - type
        - text
      title: Text
      type: object
    RequestToolResultBlock:
      additionalProperties: false
      properties:
        type:
          enum:
            - tool_result
          title: Type
          type: string
        tool_use_id:
          pattern: ^[a-zA-Z0-9_-]+$
          title: Tool Use Id
          type: string
        is_error:
          default: false
          title: Is Error
          type: boolean
        content:
          anyOf:
            - type: string
              x-stainless-skip:
                - go
            - items:
                discriminator:
                  mapping:
                    image: "#/components/schemas/RequestImageBlock"
                    text: "#/components/schemas/RequestTextBlock"
                  propertyName: type
                oneOf:
                  - $ref: "#/components/schemas/RequestTextBlock"
                  - $ref: "#/components/schemas/RequestImageBlock"
              type: array
              x-stainless-naming:
                python:
                  type_name: Content
          title: Content
      required:
        - type
        - tool_use_id
      title: Tool Result
      type: object
    RequestToolUseBlock:
      additionalProperties: false
      properties:
        type:
          enum:
            - tool_use
          title: Type
          type: string
        id:
          pattern: ^[a-zA-Z0-9_-]+$
          title: Id
          type: string
        name:
          maxLength: 64
          minLength: 1
          pattern: ^[a-zA-Z0-9_-]{1,64}$
          title: Name
          type: string
        input:
          title: Input
          type: object
      required:
        - type
        - id
        - name
        - input
      title: Tool Use
      type: object
    ResponseTextBlock:
      properties:
        type:
          default: text
          enum:
            - text
          title: Type
          type: string
        text:
          maxLength: 5000000
          minLength: 0
          title: Text
          type: string
      required:
        - type
        - text
      title: Text
      type: object
    ResponseToolUseBlock:
      properties:
        type:
          default: tool_use
          enum:
            - tool_use
          title: Type
          type: string
        id:
          pattern: ^[a-zA-Z0-9_-]+$
          title: Id
          type: string
        name:
          minLength: 1
          title: Name
          type: string
        input:
          title: Input
          type: object
      required:
        - type
        - id
        - name
        - input
      title: Tool Use
      type: object
    Tool:
      additionalProperties: false
      properties:
        description:
          description: |-
            Description of what this tool does.

            Tool descriptions should be as detailed as possible. The more information that the model has about what the tool is and how to use it, the better it will perform. You can use natural language descriptions to reinforce important aspects of the tool input JSON schema.
          example:
            - Get the current weather in a given location
          title: Description
          type: string
        name:
          maxLength: 64
          minLength: 1
          pattern: ^[a-zA-Z0-9_-]{1,64}$
          title: Name
          type: string
        input_schema:
          allOf:
            - $ref: "#/components/schemas/InputSchema"
          description: |-
            [JSON schema](https://json-schema.org/) for this tool's input.

            This defines the shape of the `input` that your tool accepts and that the model will produce.
          example:
            - properties:
                location:
                  description: The city and state, e.g. San Francisco, CA
                  type: string
                unit:
                  description: Unit for the output - one of (celsius, fahrenheit)
                  type: string
              required:
                - location
              type: object
      required:
        - name
        - input_schema
      title: Tool
      type: object
    ToolChoiceAny:
      additionalProperties: false
      description: The model will use any available tools.
      properties:
        type:
          enum:
            - any
          title: Type
          type: string
      required:
        - type
      title: ToolChoiceAny
      type: object
    ToolChoiceAuto:
      additionalProperties: false
      description: The model will automatically decide whether to use tools.
      properties:
        type:
          enum:
            - auto
          title: Type
          type: string
      required:
        - type
      title: ToolChoiceAuto
      type: object
    ToolChoiceTool:
      additionalProperties: false
      description: The model will use the specified tool with `tool_choice.name`.
      properties:
        type:
          enum:
            - tool
          title: Type
          type: string
        name:
          description: The name of the tool to use.
          title: Name
          type: string
      required:
        - type
        - name
      title: ToolChoiceTool
      type: object
    Usage:
      properties:
        input_tokens:
          description: The number of input tokens which were used.
          example:
            - 2095
          title: Input Tokens
          type: integer
        output_tokens:
          description: The number of output tokens which were used.
          example:
            - 503
          title: Output Tokens
          type: integer
      required:
        - input_tokens
        - output_tokens
      title: Usage
      type: object
    ContentBlock:
      discriminator:
        mapping:
          text: "#/components/schemas/ResponseTextBlock"
          tool_use: "#/components/schemas/ResponseToolUseBlock"
        propertyName: type
      oneOf:
        - $ref: "#/components/schemas/ResponseTextBlock"
        - $ref: "#/components/schemas/ResponseToolUseBlock"
    Model:
      title: Model
      description: The model that will complete your prompt.
        See [models](https://docs.anthropic.com/en/docs/models-overview) for additional details and options.
      anyOf:
        - type: string
          enum:
            - claude-3-5-sonnet-20240620
            - claude-3-opus-20240229
            - claude-3-haiku-20240307
          x-enum-descriptions:
            - Our most intelligent model
            - Excels at writing and complex tasks
            - Balance of speed and intelligence
            - Fast and cost-effective
            - null
            - null
            - null
          x-stainless-nominal: false
    #============== Assistant schemas ==============
    Assistant:
      type: object
      required:
        - name
        - instructions
      properties:
        name:
          type: string
          maxLength: 64
        description:
          type: string
          maxLength: 256
        instructions:
          type: string
          maxLength: 16384
        temperature:
          type: number
          minimum: 0
          maximum: 1
        model:
          type: string
          maxLength: 64
        capabilities:
          type: object
          properties:
            webSearch:
              type: boolean
            dataAnalyst:
              type: boolean
            imageGeneration:
              type: boolean
        actions:
          type: array
          items:
            $ref: "#/components/schemas/Action"
        vectorDb:
          type: array
          items:
            $ref: "#/components/schemas/VectorDb"
        knowledgeFolderIds:
          type: array
          items:
            type: string
        attachmentIds:
          type: array
          items:
            type: string
            format: uuid
          description: Array of UUID strings identifying attachments for this message
    #============== Mistral schemas ==============
    AssistantMessage:
      properties:
        content:
          title: Content
          anyOf:
            - type: string
            - items:
                $ref: '#/components/schemas/ContentChunk'
              type: array
        tool_calls:
          anyOf:
            - items:
                $ref: '#/components/schemas/ToolCall'
              type: array
          title: Tool Calls
        prefix:
          type: boolean
          title: Prefix
          default: false
        role:
          type: string
          default: assistant
          title: Role
          enum:
            - assistant
      additionalProperties: false
      type: object
      title: AssistantMessage
    FIMCompletionRequest:
      properties:
        model:
          anyOf:
            - type: string
          title: Model
          default: codestral-2405
          description: |-
            ID of the model to use. Only compatible for now with:
              - `codestral-2405`
        temperature:
          anyOf:
            - type: number
              maximum: 1.5
              minimum: 0
          title: Temperature
          description: What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value.
        top_p:
          type: number
          maximum: 1
          minimum: 0
          title: Top P
          default: 1
          description: Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.
        max_tokens:
          anyOf:
            - type: integer
              minimum: 0
          title: Max Tokens
          description: The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.
        stream:
          type: boolean
          title: Stream
          default: false
          description: 'Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON.'
        stop:
          anyOf:
            - type: string
            - items:
                type: string
              type: array
          title: Stop
          description: Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
        random_seed:
          anyOf:
            - type: integer
              minimum: 0
          title: Random Seed
          description: The seed to use for random sampling. If set, different calls will generate deterministic results.
        prompt:
          type: string
          title: Prompt
          description: The text/code to complete.
        suffix:
          anyOf:
            - type: string
          title: Suffix
          default: ''
          description: Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`.
        min_tokens:
          anyOf:
            - type: integer
              minimum: 0
          title: Min Tokens
          description: The minimum number of tokens to generate in the completion.
      additionalProperties: false
      type: object
      required:
        - prompt
        - model
      title: FIMCompletionRequest
    FunctionCall:
      properties:
        name:
          type: string
          title: Name
        arguments:
          title: Arguments
          anyOf:
            - type: object
              additionalProperties: true
            - type: string
      additionalProperties: false
      type: object
      required:
        - name
        - arguments
      title: FunctionCall
    ImageURL:
      properties:
        url:
          type: string
          title: Url
        detail:
          anyOf:
            - type: string
          title: Detail
      additionalProperties: false
      type: object
      required:
        - url
      title: ImageURL
    ImageURLChunk:
      properties:
        type:
          type: string
          enum:
            - image_url
          title: Type
          default: image_url
        image_url:
          anyOf:
            - $ref: '#/components/schemas/ImageURL'
            - type: string
          title: Image Url
      additionalProperties: false
      type: object
      required:
        - image_url
      title: ImageURLChunk
      description: '{"type":"image_url","image_url":{"url":"data:image/png;base64,iVBORw0'
    TextChunk:
      properties:
        type:
          type: string
          enum:
            - text
          title: Type
          default: text
        text:
          type: string
          title: Text
      additionalProperties: false
      type: object
      required:
        - text
      title: TextChunk
    ToolCall:
      properties:
        id:
          type: string
          title: Id
          default: 'null'
        type:
          $ref: '#/components/schemas/ToolTypes'
          default: function
        function:
          $ref: '#/components/schemas/FunctionCall'
      additionalProperties: false
      type: object
      required:
        - function
      title: ToolCall
    ToolTypes:
      type: string
      enum:
        - function
      title: ToolTypes
    ContentChunk:
      oneOf:
        - $ref: '#/components/schemas/TextChunk'
        - $ref: '#/components/schemas/ImageURLChunk'
      discriminator:
        propertyName: type
        mapping:
          image_url: '#/components/schemas/ImageURLChunk'
          text: '#/components/schemas/TextChunk'
      title: ContentChunk
    UsageInfo:
      title: UsageInfo
      type: object
      properties:
        prompt_tokens:
          type: integer
          example: 16
        completion_tokens:
          type: integer
          example: 34
        total_tokens:
          type: integer
          example: 50
      required:
        - prompt_tokens
        - completion_tokens
        - total_tokens
    ResponseBase:
      type: object
      title: ResponseBase
      properties:
        id:
          type: string
          example: cmpl-e5cc70bb28c444948073e77776eb30ef
        object:
          type: string
          example: chat.completion
        model:
          type: string
          example: mistral-small-latest
        usage:
          $ref: '#/components/schemas/UsageInfo'
    ChatCompletionChoice:
      title: ChatCompletionChoice
      type: object
      required:
        - index
        - finish_reason
        - message
      properties:
        index:
          type: integer
          example: 0
        message:
          $ref: '#/components/schemas/AssistantMessage'
        finish_reason:
          type: string
          enum:
            - stop
            - length
            - model_length
            - error
            - tool_calls
          example: stop
    DeltaMessage:
      title: DeltaMessage
      type: object
      properties:
        role:
          anyOf:
            - type: string
        content:
          anyOf:
            - type: string
            - items:
                $ref: '#/components/schemas/ContentChunk'
              type: array
        tool_calls:
          anyOf:
            - type: array
              items:
                $ref: '#/components/schemas/ToolCall'
    ChatCompletionResponseBase:
      allOf:
        - $ref: '#/components/schemas/ResponseBase'
        - type: object
          title: ChatCompletionResponseBase
          properties:
            created:
              type: integer
              example: 1702256327
    ChatCompletionResponse:
      allOf:
        - $ref: '#/components/schemas/ChatCompletionResponseBase'
        - type: object
          title: ChatCompletionResponse
          properties:
            choices:
              type: array
              items:
                $ref: '#/components/schemas/ChatCompletionChoice'
          required:
            - id
            - object
            - data
            - model
            - usage
    FIMCompletionResponse:
      allOf:
        - $ref: '#/components/schemas/ChatCompletionResponse'
        - type: object
          properties:
            model:
              type: string
              example: codestral-latest
    CompletionEvent:
      title: CompletionEvent
      type: object
      required:
        - data
      properties:
        data:
          $ref: '#/components/schemas/CompletionChunk'
    CompletionChunk:
      title: CompletionChunk
      type: object
      required:
        - id
        - model
        - choices
      properties:
        id:
          type: string
        object:
          type: string
        created:
          type: integer
        model:
          type: string
        usage:
          $ref: '#/components/schemas/UsageInfo'
        choices:
          type: array
          items:
            $ref: '#/components/schemas/CompletionResponseStreamChoice'
    CompletionResponseStreamChoice:
      title: CompletionResponseStreamChoice
      type: object
      required:
        - index
        - delta
        - finish_reason
      properties:
        index:
          type: integer
        delta:
          $ref: '#/components/schemas/DeltaMessage'
        finish_reason:
          type: string
          enum:
            - stop
            - length
            - error
            - tool_calls
